{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom transformers import pipeline\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = pipeline(\"text-generation\", model=\"unsloth/Llama-3.2-1B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmessages = [\n    {\"role\":\"system\", \"content\":  \"You are a highly logical AI. Think step by step and explain your reasoning clearly.\"},\n    {\"role\": \"user\", \"content\": \"A 5-year-old boy presents with bleeding gums, malnutrition, and small, curled hairs, while having a normal CBC except for mild anemia with an MCV of 85. His diet primarily consists of boiled and preserved vegetables due to financial hardship. What nutritional deficiency is most likely responsible for his symptoms?\"},\n]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\noutputs = pipe(prompt, max_new_tokens=1024)\n\nprint(outputs[0][\"generated_text\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport torch\nassert (torch.cuda.is_available()==True)\n\n\n!pip install --no-build-isolation axolotl[flash-attn,deepspeed]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers==4.47.1 --force-reinstall\n!pip install numpy==2.1 --force-reinstall\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom datasets import load_dataset\n\n# Load dataset\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n\n# Convert dataset to Alpaca format\nformatted_data = []\nfor example in dataset[\"train\"]:\n    formatted_data.append({\n        \"instruction\": \"Analyze the following medical case and provide step-by-step reasoning before giving a diagnosis.\",\n        \"input\": example[\"Question\"],\n        \"output\": f\"Let's think step by step. {example['Complex_CoT']} Response: {example['Response']}\"\n    })\n\n# Save as JSONL\nwith open(\"medical_reasoning_alpaca.jsonl\", \"w\") as f:\n    for entry in formatted_data:\n        f.write(json.dumps(entry) + \"\\n\")\n\nprint(\"Alpaca-style dataset saved!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # import datasets\n\n# # # Load dataset from Hugging Face\n# # dataset = datasets.load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n\n# # # Save dataset to disk\n# # dataset.save_to_disk(\"/content/my_dataset\")\n# # print(\"Dataset saved successfully!\")\n# import datasets\n\n# # Load dataset from saved location\n# dataset = datasets.load_from_disk(\"medical_reasoning_alpaca.jsonl\")\n\n# # Print column names to verify\n# print(dataset.column_names)\n\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load JSONL dataset properly\ndataset = load_dataset(\"json\", data_files=\"medical_reasoning_alpaca.jsonl\")\n\n# Print column names to verify\nprint(dataset.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:05:35.343763Z","iopub.execute_input":"2025-02-21T14:05:35.344090Z","iopub.status.idle":"2025-02-21T14:05:35.689539Z","shell.execute_reply.started":"2025-02-21T14:05:35.344066Z","shell.execute_reply":"2025-02-21T14:05:35.688745Z"}},"outputs":[{"name":"stdout","text":"{'train': ['instruction', 'input', 'output']}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import yaml\n\ntrain_config = {\n    \"base_model\": \"unsloth/Llama-3.2-1B-Instruct\",\n\n    \n    \"datasets\": [\n        {\n            \"path\": \"medical_reasoning_alpaca.jsonl\",  \n            \"type\": \"alpaca\" \n        }\n    ],\n\n   \n    \"output_dir\": \"./models/Llama3_medical_Reasoning\",\n\n    \n    \"sequence_length\": 2048,  # Increased for long medical reasoning responses\n   \"bf16\": \"auto\",\n    \"tf32\": False,\n\n    # training params\n    \"micro_batch_size\": 2,\n    \"num_epochs\": 2,\n    \"optimizer\": \"adamw_bnb_8bit\",\n    \"learning_rate\": 0.0001,\n\n    \"logging_steps\": 1,\n\n    # LoRA / qLoRA\n    \"adapter\": \"qlora\",\n    \"lora_r\": 32,\n    \"lora_alpha\": 16,\n    \"lora_dropout\": 0.05,\n    \"lora_target_linear\": True,\n\n    # Gradient Accumulation\n    \"gradient_accumulation_steps\": 1,\n\n    # Gradient Checkpointing\n    \"gradient_checkpointing\": True,\n\n    # Low Precision\n    \"load_in_8bit\": False,\n    \"load_in_4bit\": True,\n\n    # Flash Attention\n    \"flash_attention\": False,\n    # # Train on Inputs\n    # \"train_on_inputs\": False,\n}\n\n# Write the YAML file\nwith open(\"specialised_train.yml\", 'w') as file:\n    yaml.dump(train_config, file)\n\nprint(\"Updated training configuration saved as specialised_train.yml!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall numpy -y\n!pip install numpy==1.26.4\n# sudo systemctl restart nv-peer-mem\n# sudo systemctl restart nvidia-persistenced\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --no-build-isolation torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall transformers accelerate bitsandbytes axolotl -y\n!pip install --no-build-isolation axolotl[flash-attn,deepspeed]\n!pip install transformers==4.47.1 accelerate==0.26.1 bitsandbytes==0.42.0\n!pip install flash-attn\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch -m axolotl.cli.train specialised_train.yml\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch -m axolotl.cli.merge_lora specialised_train.yml\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query=dataset['train'][3]['Question']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install bitsandbytes\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n#AreejMehboob17/Llama-3.2-1B-LongAlpaca\nmodel_path = \"./models/Llama3_medical_Reasoning/merged\"\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path, legacy=False)\n\n# Enable 8-bit loading to reduce VRAM usage\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\n# Load the model in 8-bit\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    quantization_config=quantization_config\n)\n\n# Initialize the text-generation pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nmessages = [\n    {\"role\":\"system\", \"content\": query+\"<END>\"},\n]\n\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\noutput = pipe(prompt)\nprint(\"Model Output:\", output[0]['generated_text'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade transformers\n!pip install transformers==4.37.0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nconfig_path = \"./models/Llama3_medical_Reasoning/merged/config.json\"\n\n# Load JSON\nwith open(config_path, \"r\") as f:\n    config = json.load(f)\n\n# Fix the rope_scaling issue\nconfig[\"rope_scaling\"] = {\n    \"type\": \"dynamic\",\n    \"factor\": 2.0\n}\n\n# Save the modified JSON\nwith open(config_path, \"w\") as f:\n    json.dump(config, f, indent=4)\n\nprint(\"✅ config.json updated successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf ./models/Llama3_medical_Reasoning/merged/tokenizer.json\n!rm -rf ./models/Llama3_medical_Reasoning/merged/tokenizer_config.json\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y unsloth transformers\n!!pip install unsloth==2024.10.4 torch==2.4.1\n!pip install transformers==4.45.2\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_path = \"./models/Llama3_medical_Reasoning/merged\"\n\n# Download a fresh tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\ntokenizer.save_pretrained(model_path)\n\nprint(\"✅ Tokenizer replaced successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pipe = pipeline(\"text-generation\", model=\"./models/Llama3_medical_Reasoning/merged\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ls -lh ./models/Llama3_medical_Reasoning/merged/\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade transformers sentencepiece unsloth\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rm -rf ./models/Llama3_medical_Reasoning/merged/tokenizer.*\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:02:10.263459Z","iopub.execute_input":"2025-02-21T14:02:10.263778Z","iopub.status.idle":"2025-02-21T14:02:10.387330Z","shell.execute_reply.started":"2025-02-21T14:02:10.263748Z","shell.execute_reply":"2025-02-21T14:02:10.386202Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_path = \"./models/Llama3_medical_Reasoning/merged\"\n\n# Force redownload a clean tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    \"unsloth/Llama-3.2-1B-Instruct\",\n    force_download=True,\n    use_fast=False  # Sometimes `use_fast=True` causes issues\n)\n\n# Save it to your model directory\ntokenizer.save_pretrained(model_path)\n\nprint(\"✅ Tokenizer replaced successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:02:11.570175Z","iopub.execute_input":"2025-02-21T14:02:11.570535Z","iopub.status.idle":"2025-02-21T14:02:17.570913Z","shell.execute_reply.started":"2025-02-21T14:02:11.570504Z","shell.execute_reply":"2025-02-21T14:02:17.570036Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b2eb553c9f64e6da4483ef955266b7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88f5aa76514e467eae140ffea2cf16b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4f750852fb3474c87f4d9faa50eecaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f982c48fb0e4cb3a7d7e8cc4552b753"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e886e3e7f54bee984ad74e28fc4f8a"}},"metadata":{}},{"name":"stdout","text":"✅ Tokenizer replaced successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"inst=dataset['train'][3]['instruction']\nquery=dataset['train'][3]['input']\nquery","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:07:11.895440Z","iopub.execute_input":"2025-02-21T14:07:11.895743Z","iopub.status.idle":"2025-02-21T14:07:11.901432Z","shell.execute_reply.started":"2025-02-21T14:07:11.895719Z","shell.execute_reply":"2025-02-21T14:07:11.900666Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'A patient with psoriasis was treated with systemic steroids, and upon discontinuation of the treatment, developed generalized pustules all over the body. What is the most likely cause of this condition?'"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset['train'][3]['output']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:07:25.298924Z","iopub.execute_input":"2025-02-21T14:07:25.299219Z","iopub.status.idle":"2025-02-21T14:07:25.305126Z","shell.execute_reply.started":"2025-02-21T14:07:25.299196Z","shell.execute_reply":"2025-02-21T14:07:25.304222Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"\"Let's think step by step. I'm thinking about this patient with psoriasis who's been on systemic steroids. Okay, so psoriasis is this chronic skin condition that can be pretty stubborn and flares up now and then. They stopped their steroid treatment, and then suddenly there are these generalized pustules appearing all over the place. What's going on here? \\n\\nAlright, pustules are like little bumps that are filled with pus—they can be a sign of different skin issues, but in someone with psoriasis, a major change like stopping treatment can really mess with things. Steroids are powerful at tamping down inflammation and managing the immune system. When you take someone off steroids, particularly suddenly, the body can react in unexpected ways. It's like the immune system gets a bit confused and overreacts—a rebound effect. \\n\\nOh, right, I remember reading that systemic steroids can sometimes mask the severity of psoriasis, keeping it under control. But pulling those steroids too fast can allow the psoriasis to come back stronger, even shifting into another form. This sounds familiar—there's a type called pustular psoriasis. If I recall, that's when the psoriasis turns into this more extreme version with pustules. \\n\\nSo connecting the dots: it's likely that stopping the steroids led to a rebound and we might be seeing a case of generalized pustular psoriasis. That makes sense given the scenario—I've seen this mentioned before where physicians need to be cautious about how they taper steroids off in psoriasis patients to prevent exactly this kind of escalation. It fits pretty well with what I'd expect based on what I know about steroid withdrawal and psoriasis flare-ups. Response: The development of generalized pustules in a patient with psoriasis following the discontinuation of systemic steroid treatment is most likely due to a condition called generalized pustular psoriasis (GPP). This condition can occur as a rebound effect when systemic steroids are suddenly stopped or rapidly tapered down. In patients with psoriasis, steroids can suppress the immune system and mitigate symptoms. When these drugs are withdrawn too quickly, the immune system can overreact, leading to more severe symptoms, including the transformation of psoriasis into a pustular form. Careful management and slow tapering of steroids are critical in psoriasis patients to prevent such exacerbations.\""},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_path = \"./models/Llama3_medical_Reasoning/merged\"\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path, legacy=False)\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\n# Initialize the text-generation pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n# Test inference\nprompt = \"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions?\"\noutput = pipe(prompt, max_length=200)\n\nprint(\"Model Output:\", output[0]['generated_text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:02:21.214672Z","iopub.execute_input":"2025-02-21T14:02:21.215154Z","iopub.status.idle":"2025-02-21T14:02:32.020202Z","shell.execute_reply.started":"2025-02-21T14:02:21.215124Z","shell.execute_reply":"2025-02-21T14:02:32.019229Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Model Output: A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most likely reveal about her residual volume and detrusor contractions? \n\n## Step 1: Understanding the scenario\nThe patient is a 61-year-old woman who has a history of involuntary urine loss during activities like coughing or sneezing. She undergoes a gynecological exam and a Q-tip test. This exam is specifically designed to assess the bladder's function by looking for any involuntary contractions during the contraction phase of urination.\n\n## Step 2: Identifying the purpose of the Q-tip test\nThe Q-tip test is used to evaluate the bladder's ability to contract and store urine. It helps in determining how well the bladder muscle is functioning, particularly in terms of its ability to hold urine and prevent leakage during voluntary\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_path = \"./models/Llama3_medical_Reasoning/merged\"\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path, legacy=False)\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\n# Initialize the text-generation pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\nmessages = [\n    {\"role\":\"system\", \"content\":  inst},\n    {\"role\": \"user\", \"content\": query}\n]\n\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n\n\noutput = pipe(prompt, max_length=1024)\n\nprint(\"Model Output:\", output[0]['generated_text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:07:57.205461Z","iopub.execute_input":"2025-02-21T14:07:57.205784Z","iopub.status.idle":"2025-02-21T14:08:12.915462Z","shell.execute_reply.started":"2025-02-21T14:07:57.205759Z","shell.execute_reply":"2025-02-21T14:08:12.914470Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Model Output: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 21 Feb 2025\n\nAnalyze the following medical case and provide step-by-step reasoning before giving a diagnosis.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nA patient with psoriasis was treated with systemic steroids, and upon discontinuation of the treatment, developed generalized pustules all over the body. What is the most likely cause of this condition?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nLet's think step by step. Alright, so we have a patient who was treated with systemic steroids for psoriasis, and now they're showing generalized pustules all over their body after stopping the treatment. That's concerning.\n\nFirst, let's think about what happens when we use steroids. They can have several effects on the body, especially on the skin. One of the things they do is suppress the immune system, which can lead to an increased risk of infections. Hmm, so this could be a big clue here.\n\nNow, when we look at the skin, pustules are a classic sign of a bacterial infection. They're like little bumps filled with pus, and they often pop and heal quickly, which can be a sign of a bacterial infection. So, it makes sense that we're dealing with a bacterial infection here.\n\nThe fact that the pustules are generalized across the body, spreading all over, is really telling us something important. This kind of widespread infection could be due to something systemic. When we're talking about systemic infections, they can spread from one place to another, and that's where the body's immune system comes in.\n\nOh, and another thing to consider is the timing. If the patient was on steroids and then stopped, it's likely that the immune system was suppressed, making them more vulnerable to infections. This fits perfectly with the scenario where the body is more susceptible to bacterial infections.\n\nSo, putting it all together, it seems like we're dealing with a bacterial infection that's systemic in nature. The combination of the generalized pustules, the timing, and the patient's previous steroid treatment make it most likely a bacterial infection like impetigo, which is a common type of bacterial skin infection.\n\nYep, that fits the picture. It's likely that the steroids caused the immune system suppression, which made the patient more susceptible to bacterial infections like impetigo. So, I'm pretty confident that impetigo is the culprit here. Response: The most likely cause of the generalized pustules that developed after stopping the steroid treatment in this patient is impetigo.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_path = \"unsloth/Llama-3.2-1B-Instruct\"\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path, legacy=False)\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\n# Initialize the text-generation pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n\nmessages = [\n    {\"role\":\"system\", \"content\":  \"You are a highly logical AI. Think step by step and explain your reasoning clearly.\"},\n    {\"role\": \"user\", \"content\": \"A 59-year-old man presents with abdominal pain, severe vomiting, a recent tonic-clonic seizure, and ventricular tachycardia after ingesting an unknown medication in a suicide attempt. He has a medical history of COPD, coronary artery disease, and chronic back pain. Laboratory tests show hyperglycemia and hypokalemia. Considering these clinical findings, which drug overdose is most likely responsible for his current condition?\"}\n]\n\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n\n\noutput = pipe(prompt, max_length=1024)\n\nprint(\"Model Output:\", output[0]['generated_text'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install huggingface_hub transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:11:00.140566Z","iopub.execute_input":"2025-02-21T14:11:00.140946Z","iopub.status.idle":"2025-02-21T14:11:04.465342Z","shell.execute_reply.started":"2025-02-21T14:11:00.140919Z","shell.execute_reply":"2025-02-21T14:11:04.464083Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.28.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.49.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!huggingface-cli login --token your_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:19:07.900976Z","iopub.execute_input":"2025-02-21T14:19:07.901311Z","iopub.status.idle":"2025-02-21T14:19:08.750141Z","shell.execute_reply.started":"2025-02-21T14:19:07.901285Z","shell.execute_reply":"2025-02-21T14:19:08.749055Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nThe token `write_access_token` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `write_access_token`\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"!huggingface-cli repo create Llama-3-1B-MedCot-Instruct -y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:22:31.662616Z","iopub.execute_input":"2025-02-21T14:22:31.663017Z","iopub.status.idle":"2025-02-21T14:22:32.861603Z","shell.execute_reply.started":"2025-02-21T14:22:31.662983Z","shell.execute_reply":"2025-02-21T14:22:32.860265Z"}},"outputs":[{"name":"stdout","text":"\u001b[90mgit version 2.34.1\u001b[0m\n\u001b[90mgit-lfs/3.0.2 (GitHub; linux amd64; go 1.18.1)\u001b[0m\n\nYou are about to create \u001b[1mAreejMehboob17/Llama-3-1B-MedCot-Instruct\u001b[0m\n\nYour repo now lives at:\n  \u001b[1mhttps://huggingface.co/AreejMehboob17/Llama-3-1B-MedCot-Instruct\u001b[0m\n\nYou can clone it locally with the command below, and commit/push as usual.\n\n  git clone https://huggingface.co/AreejMehboob17/Llama-3-1B-MedCot-Instruct\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"from huggingface_hub import upload_folder\n\nupload_folder(\n    folder_path=\"./models/Llama3_medical_Reasoning/merged\",  # Path to your model directory\n    repo_id=\"AreejMehboob17/Llama-3-1B-MedCot-Instruct\",  # Your Hugging Face repo\n    repo_type=\"model\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:23:59.990355Z","iopub.execute_input":"2025-02-21T14:23:59.990804Z","iopub.status.idle":"2025-02-21T14:25:03.938271Z","shell.execute_reply.started":"2025-02-21T14:23:59.990770Z","shell.execute_reply":"2025-02-21T14:25:03.937216Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76d82f72874b4815a2fa7ba89a92f2c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bbe1287ff024d7b9d80a18e50a814c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a25ad61ec8e24e4abacf9e02c760bff7"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/AreejMehboob17/Llama-3-1B-MedCot-Instruct/commit/f953c0b0284174beafe9b0e5d3fa5ca6f2a0f816', commit_message='Upload folder using huggingface_hub', commit_description='', oid='f953c0b0284174beafe9b0e5d3fa5ca6f2a0f816', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AreejMehboob17/Llama-3-1B-MedCot-Instruct', endpoint='https://huggingface.co', repo_type='model', repo_id='AreejMehboob17/Llama-3-1B-MedCot-Instruct'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_path = \"AreejMehboob17/Llama-3-1B-MedCot-Instruct\"\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path, legacy=False)\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\"\n)\n\n# Initialize the text-generation pipeline\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:25:49.278858Z","iopub.execute_input":"2025-02-21T14:25:49.279233Z","iopub.status.idle":"2025-02-21T14:27:05.555390Z","shell.execute_reply.started":"2025-02-21T14:25:49.279204Z","shell.execute_reply":"2025-02-21T14:27:05.554312Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"394c07344bae40618d4a43491b4115ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1885fadb388f47579e1433efa27dd126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b9673c1d16847429ecc65e10e820621"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/942 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65818c6c07e143be8742f29af2d565fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af8fcedf91d499a9331d0709441916e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03940a6614ba451cbc5e5ff0b8d49fdf"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Model Output: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 21 Feb 2025\n\nAnalyze the following medical case and provide step-by-step reasoning before giving a diagnosis.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nA patient with psoriasis was treated with systemic steroids, and upon discontinuation of the treatment, developed generalized pustules all over the body. What is the most likely cause of this condition?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nLet's think step by step. Alright, so we've got a patient who had psoriasis and was treated with systemic steroids. That's interesting because steroids can really dry out the skin, which makes it more susceptible to infections. Now, when we stop using steroids, the skin's barrier function is restored, and it's more open to bacteria and other pathogens. This is known as the \"steroid withdrawal effect.\"\n\nWhen the skin barrier is down, it's like leaving a window open to everything that might come in, and that's exactly what's happening here. So, the skin is now more vulnerable to infections. And, if we're talking about generalized pustules all over the body, it's like the body's immune system is reacting to something foreign by forming these little bumps.\n\nNow, I know there are several things that could cause these kinds of reactions, but the key here is the timing. The pustules appeared after stopping the steroids, which makes me think of a few possibilities. \n\nOne of the big culprits is a bacterial infection. Given the timing and the skin reaction, it's pretty common for bacterial infections to take over when the skin barrier is down, especially with things like impetigo or cellulitis. \n\nSo, putting it all together, I'd say it's likely that the patient got a bacterial infection. It's pretty common to see this kind of reaction when you stop steroids and the skin is exposed to the outside environment. \n\nThat's why I'd say the most likely cause here is a bacterial infection. It makes sense with the timing and the symptoms. Yeah, I'd go with that.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"messages = [\n    {\"role\":\"system\", \"content\":  inst},\n    {\"role\": \"user\", \"content\": \"A 33-year-old woman presents with a one-month history of non-bloody nipple discharge and bilateral galactorrhea upon palpation. She has a history of smoking, alcohol use, and is receiving psychiatric treatment, although she cannot recall the medication name. What is the best next step in management for investigating the cause of her symptoms?\"}\n]\n\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n\n\noutput = pipe(prompt)\n\nprint(\"Model Output:\", output[0]['generated_text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T14:36:01.074190Z","iopub.execute_input":"2025-02-21T14:36:01.074623Z","iopub.status.idle":"2025-02-21T14:36:16.086079Z","shell.execute_reply.started":"2025-02-21T14:36:01.074591Z","shell.execute_reply":"2025-02-21T14:36:16.084986Z"}},"outputs":[{"name":"stdout","text":"Model Output: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 21 Feb 2025\n\nAnalyze the following medical case and provide step-by-step reasoning before giving a diagnosis.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nA 33-year-old woman presents with a one-month history of non-bloody nipple discharge and bilateral galactorrhea upon palpation. She has a history of smoking, alcohol use, and is receiving psychiatric treatment, although she cannot recall the medication name. What is the best next step in management for investigating the cause of her symptoms?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nLet's think step by step. Alright, let's think this through step by step. So, we have a 33-year-old woman with a month-long history of bloody nipple discharge and galactorrhea. That's interesting because both of these symptoms are quite unique. Now, she has a history of smoking, which is a big clue. Smoking can lead to conditions like hypothyroidism, which often causes galactorrhea. \n\nNow, she's also on psychiatric treatment, which could be important because certain medications can cause galactorrhea. Hmm, I'm not sure what her medication is, but I know some psychiatric treatments can lead to this side effect.\n\nShe can't remember the name of her medication, which makes it tough to pinpoint the exact cause. So, what's next? \n\nFirst, I should probably consider checking her thyroid function. Since she has a history of hypothyroidism, I'd want to make sure everything is in check. I'd probably want to check her TSH and T4 levels. This will help rule out hypothyroidism.\n\nOh, and considering her symptoms could be related to hypothyroidism, checking for thyroid nodules or goiter might be a good idea too. A thyroid nodule could cause galactorrhea, especially if it's related to hyperthyroidism.\n\nSo, I think the best step here is to get her thyroid function checked first. That would give us a solid foundation to work with. \n\nAnd, since she's on psychiatric treatment, it's good to consider checking for medications that could be causing galactorrhea, but we should prioritize the thyroid first. \n\nSo, yeah, I'd definitely recommend getting her thyroid function checked. That's the next logical step. Response: The best next step in managing this patient is to investigate her thyroid function, given her symptoms and her history. This will help determine if she has hypothyroidism, which could be the underlying cause of her galactorrhea and blood-tinged nipple discharge.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}